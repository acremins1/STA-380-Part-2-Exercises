---
title: 'STA 380 Part 2: Exercises'
author: "Aidan Cremins, Peyton Lewis, Joe Morris, Amrit Sandhu"
date: '2022-07-29'
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r,include=FALSE}
library(dplyr)
library(ggplot2)
library(forcats)
library(reshape2)
library(knitr)
library(mosaic)
library(quantmod)
library(foreach)
```

# Probability Practice

### Part a.

P(Y) = 0.65
P(N) = 0.35
P(RC) = 0.3
P(TC) = 0.7 (P(RC)-1)
P(Y|RC) = 0.5
P(N|RC) = 0.5

These probabilities are summarized in the table below:

![alt](Table.png)

We're looking for P(Y|TC) so we can use the rule of total probability:

P(Y) = P(Y, TC) + P(Y, RC) = P(TC) * P(Y|TC) + P(RC) * P(Y|RC)

We know all of these inputs to the equation except for P(Y|TC), so we want to solve for that unknown.

0.65 = 0.7 * P(Y|TC) + 0.3 * 0.5

From the above equation, we find that P(Y|TC) $\approx$ 0.714286. This means that truthful clickers answer yes to the question about 71.43% of the time.

### Part b.

P(Disease) = 0.000025
P(No Disease) = 0.999975 (1-0.000025)
P(Positive|Disease) = .993
P(Negative|No Disease) = 0.9999

The probabilities above are summarized in the tree diagram below:

![alt](tree.png)

We're looking for P(Disease|Positive) so we can use Baye's Law:

$\frac{P(Disease)*P(Positive|Disease)}{P(Disease)*P(Positive|Disease)+P(No Disease)*P(Positive|No Disease)}$

We have almost all of the inputs that we need, however, we're missing P(Positive|No Disease). These are false positives. We can find the missing probability by taking 1 - true negatives, or 1 - 0.9999 to get P(Positive|No Disease) as 0.0001. Now we can solve for P(Disease|Positive).

$\frac{0.000025*0.993}{0.000025*0.993+0.999975*0.0001}$ $\approx$ .198882. Thus, if someone tests positive, they have about a 19.89% chance of actually having the disease.

# Wrangling the Billboard Top 100

```{r,include=FALSE}
billboard = read.csv("data/billboard.csv")
```

### Part a.

```{r,echo=FALSE}
billboard %>%
  group_by(performer,song) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  head(10)
```

The table above shows the top 10 longest lasting songs on the Billboard 100. It reveals that a majority of these long-lasting songs are more recent songs.

### Part b.

```{r,echo=FALSE,warnings=FALSE}
musical_diversity = billboard %>%
  filter(year != 1958 & year != 2021) %>%
  group_by(year) %>%
  summarize(song_performer = paste(performer, song),unique_songs_per_year=length(unique(song_performer)))

ggplot(musical_diversity) + geom_line(aes(x = year, y = unique_songs_per_year)) + xlab("Year") + ylab("Number of Unique Songs") + ggtitle("Number of Unique Songs per Year")
```

As seen in the plot, the number of unique songs steadily decreased from roughly 1965 to a overall minimum of just below 400 songs around 2000. Since then, the trend has been increasing and in the last year of data (2020), there were about 800 unique songs. It looks like it's on trend to surpass its historical max in the mid 1960s (about 830).

### Part c.

```{r}
ten_week_hit_songs <- billboard %>%
  group_by(performer,song) %>%
  summarize(ten_week_hit = ifelse(n()>=10,"Yes","No")) %>%
  filter(ten_week_hit == "Yes")

top_artists <- ten_week_hit_songs %>%
  group_by(performer) %>%
  summarize(num_ten_week_hit = n()) %>%
  filter(num_ten_week_hit>=30)

ggplot(top_artists) + geom_bar(aes(x = fct_reorder(performer,num_ten_week_hit), y = num_ten_week_hit),stat = "identity",fill="cornflowerblue") + coord_flip() + xlab("Artist Name") + ylab("Number of Ten Week Hits") + ggtitle("Artists With at Least 30 Ten Week Hits")
```

Given the bar plot above, it shows us that Elton John had the most 10 week hits with just over 50. The other 18 artists in the plot seem to be a mix of generations and genres, but they all are relatively well known which makes sense given that they have so many hits.

#Visual story telling part 1: green buildings

```{r,include=FALSE}
green_buildings = read.csv("data/greenbuildings.csv")
green_buildings$green_rating <- ifelse(green_buildings$green_rating==0,"Non-Green","Green")
```

```{r,include=FALSE}
#Look at correlations with green_rating, to get an initial idea of what variables to look at
#cor(green_buildings['green_rating'],green_buildings[! colnames(green_buildings) %in% c('green_rating')])
```

To begin this problem, we can look at the distribution of rents for green buildings vs. non-green buildings and we see that indeed green buildings have a slightly higher median rent than non-green buildings. We're already skeptical of the stats guru, because while yes, there is a difference in rents, it's really not very significant.

*Figure 1*
```{r,echo=FALSE}
ggplot(green_buildings, aes(x = green_rating, y = Rent ,group = green_rating)) + geom_boxplot() + xlab("Green Rating") + ggtitle("Rent vs. Green Rating")
```

Even though the rent premium wasn't that large, if the leasing rate in green buildings was substantially higher, the larger volume of tenants to charge rent to might still justify undergoing the investment. However, the boxplot below suggests that there's not a huge difference in the median leasing rates between green and non-green buildings.

*Figure 2*
```{r,echo=FALSE}
ggplot(green_buildings, aes(x = green_rating, y = leasing_rate,group = green_rating)) + geom_boxplot() + xlab("Green Rating") + ggtitle("Rent vs. Green Rating") +ylab("Leasing Rate") + ggtitle("Leasing Rate vs. Green Rating")
```

We also wanted to explore some confounding variables that may be influencing the relationship between rent and green rating. The plot below reveals that green buildings tend to be significantly newer compared to non-green buildings. This could be a problem because perhaps the premium of green buildings is just because they're newer buildings.

*Figure 3*
```{r}
ggplot(green_buildings, aes(x = green_rating, y = age, group = green_rating)) + geom_boxplot() + xlab("Green Rating") + ylab("Building Age") + ggtitle("Building Age vs. Green Rating")
```

The plot below "adjusts" for age by plotting the 10 year rolling average rent vs. age and then faceted by green vs. non-green buildings. It shows that the average rents between green vs. non-green buildings doesn't appear to be significantly different in the range of 0-100 years of age. Interestingly, we found that there are some very old (100-150 years) non-green buildings that have very low 10 year rolling average rents whereas there are no green buildings in this age range. It seems as though the very old non-green buildings may be the reason for the median rent discrepancy shown in Figure 1.

*Figure 4*
```{r}
#Get rolling average of rents for each decade 
green_buildings$decade <- (green_buildings$age%/%10)*10
decade_data <- green_buildings %>% 
  group_by(decade) %>% 
  mutate(rec = 1) %>% 
  mutate(rollavg = cumsum(Rent)/cumsum(rec)) %>% 
  select(-rec)
labels = c("Green","Not Green")
ggplot(decade_data, aes(x = age, y = rollavg)) + geom_line() + facet_grid(. ~ green_rating) + xlab("Building Age") + ylab("10 Year Rolling Average Rent")
```

Another confounding variable that we noticed was building class. There are many more buildings of class A (higher quality) than class B for green buildings and the opposite for non-green. Thus, the rent discrepancy in Figure 1 could just be because of quality differences.

*Figure 5*
```{r,echo=FALSE}
green_buildings$class <- ifelse(green_buildings$class_a,"A","B")
ggplot(green_buildings, aes(x = class)) + geom_bar(stat="count") + facet_grid(.~green_rating) + xlab("Class") + ylab("Count") + ggtitle("Frequencies of Green vs. Non-Green Buildings by Class")
```

Figures 6 and 7 below "adjust" for class by having a pair of box plots showing rent vs. green rating for class A buildings and then another set of box plots for class B buildings. After controlling for building class, in both plots the median rent for green and non-green buildings were not significantly different. 

*Figure 6*
```{r}
classa_subset <- subset(green_buildings, green_buildings$class=="A")
classb_subset <- subset(green_buildings,green_buildings$class=="B")
ggplot(classa_subset, aes(x = green_rating, y = Rent ,group = green_rating)) + geom_boxplot() + xlab("Green Rating") + ggtitle("Rent vs. Green Rating - Class A")
```

*Figure 7*
```{r}
ggplot(classb_subset, aes(x = green_rating, y = Rent ,group = green_rating)) + geom_boxplot() + xlab("Green Rating") + ggtitle("Rent vs. Green Rating - Class B")
```

Overall, there doesn't seem to be a significant difference in median rents between green and non-green buildings which was true even after controlling for building age and building class. If we were to get a different sample of green vs. non-green buildings, it's possible that the median rent for green buildings could be lower than non-green buildings because the medians in our sample were so close. Ultimately, it doesn't seem to be wise to undergo additional costs to make the building green when there doesn't appear to be a significantly higher median rent that can be charged to offset the cost. The analyst failed to consider the sample variability, confounding variables, or the time value of money (i.e. \$650,000 9 years from now isn't same as \$650,000 today).

# Visual story telling part 2: Cap Metro data

```{r,include=FALSE}
cap_metro <- read.csv("data/capmetro_UT.csv")
```

To begin, we analyzed the total volume of ridership (Boarding and Alighting) across three distinct periods of the day (Morning, Afternoon, and Evening) on both weekdays and weekends. We noticed that on weekdays, Morning and Afternoon were by far the most active times. Meanwhile, on weekends, afternoon was the most popular time followed by evening. However, the total volume of ridership was much lower on weekends as expected since people are using the Metro for leisure rather than commuting.

*Figure 1*
```{r,echo=FALSE}
cap_metro$time_of_day = ifelse(cap_metro$hour_of_day %in% c(6,7,8,9,10,11),"Morning",ifelse(cap_metro$hour_of_day %in% c(12,13,14,15,16),"Afternoon","Evening"))
time_of_day_order <- c("Morning","Afternoon","Evening")
cap_metro$activity = cap_metro$boarding + cap_metro$alighting
ggplot(cap_metro, aes(x = factor(time_of_day,levels=time_of_day_order), y = activity)) + geom_bar(stat="identity") + facet_grid(. ~ weekend) + xlab("Time of Day") + ylab("Activity (Boarding and Alighting)") +ggtitle("Volume of Ridership: Weekday vs. Weekend")
```

Following our analysis above, we decided to hone in on a specific weekday (Wednesday) and a specific weekend day (Sunday) to get a better understanding of the hourly trends of net ridership. What we discovered was net ridership was most negative (i.e. more people getting off the bus) in the morning hours of 8-9am. From here, average net ridership increases and becomes positive at around noon. It reaches its max between 4-5pm (more people getting on bus to go home).

*Figure 2*
```{r,echo=FALSE}
wednesdays = subset(cap_metro,cap_metro$day_of_week=="Wed")
ggplot(wednesdays, aes(x = hour_of_day, y = (boarding-alighting))) + geom_point() + xlab("Hour of Day") + ylab("Net Ridership") + ggtitle("Net Ridership vs. Hour of Day on Wednesdays")+geom_smooth(se=FALSE)
```

On Sundays, the trend of average net ridership is much flatter and less volatile. It appears to reach a minimum at noon, hits 0 at 3pm, and reaches its max at around 5-6. Since people aren't typically commuting to work/school on Sundays, net ridership doesn't show nearly as clear of a pattern compared to Wednesday.

*Figure 3*
```{r,echo=FALSE}
sundays = subset(cap_metro,cap_metro$day_of_week=="Sun")
ggplot(sundays, aes(x = hour_of_day, y = (boarding-alighting))) + geom_point() + xlab("Hour of Day") + ylab("Net Ridership") + ggtitle("Net Ridership vs. Hour of Day on Sundays")+geom_smooth(se=FALSE)
```

After looking at Figures 1-3, it's clear that afternoons are the most popular boarding time for both weekdays and weekends (positive net ridership). We wanted to see specifically for afternoons, what impact temperature had on curbing or enhancing Metro ridership. 

*Figure 4*
```{r}
afternoon <- subset(cap_metro, cap_metro$time_of_day=="Afternoon")
riders_temp = afternoon %>%
  group_by(timestamp) %>%
  summarize(total_riders = sum(boarding), mean_temp = mean(temperature), weekend = weekend, time_of_day=time_of_day)
ggplot(riders_temp, aes(x = mean_temp, y = total_riders)) + geom_point()+facet_grid(.~weekend)+geom_smooth(se=FALSE) + xlab("Mean Temperature") + ylab("Total Boarding") + ggtitle("Afternoon Ridership vs. Temperature")
```

The boarding trend is pretty flat on weekends, but shows an upward curve for more extreme temperatures on weekdays. The heightened sensitivity to temperature on weekday ridership is likely due to people opting for a temperature-controlled bus ride rather than being outdoors. It's also possible that the increased boarding for higher temperatures corresponds to the peak boarding times on weekdays of 3-4pm which also happens to be the hottest part of the day generally.

# Clustering and PCA

```{r,include=FALSE}
wine <- read.csv("data/wine.csv")
```

```{r,include=FALSE}
set.seed(1)
wine_quant <- wine[,! names(wine) %in% c("color","quality")]
wine_pca = prcomp(wine_quant, rank=5, scale=TRUE)
wine_copy = wine
wine_copy$pc1 <- wine_pca$x[,1]
cor_df_pc1 <- melt(as.data.frame(cor(wine_copy$pc1,wine_quant)))
colnames(cor_df_pc1) <- c("Variable","Correlation with PC1")
```

To start this analysis, we created 5 principal components based on the 11 chemical properties in the data set. To see how well our principal components captured variability in our data set, we made the scree plot below to see the percentage variance explained by each principal component. We see that principal components 1 and 2 both explain more than 20% of the variance which suggests that they're worth looking into further.

```{r,echo=FALSE}
var_explained_df <- data.frame(PC= paste0("PC",1:11),  var_explained=(wine_pca$sdev)^2/sum((wine_pca$sdev)^2))
var_explained_df = var_explained_df[1:5,]
var_explained_df %>%
  ggplot(aes(PC,var_explained))+geom_col() + xlab("Principal Component") + ylab("% Variance Explained") + ggtitle("% Variance Explained by Each Principal Component")
```

To understand what the first principal component is capturing, we looked at its correlations with other variables in the data set. We see that this principal component has strong positive correlations with: total.sulfur.dioxide, free.sulfur.dioxide, and residual sugar. It also has strong negative correlations with: volatile.acidity, sulfates, and chlorides. If we knew more about wine chemistry, this would probably become more meaningful to us.

```{r,echo=FALSE}
kable(cor_df_pc1)
```

*Figure 1*

We plotted the wine color against the scores of the first principal component to see whether this principal component was useful in separating between red and white wines. Given the differences in the box plots' medians, the first principal component is useful in determining color.

*Figure 2*
```{r,echo=FALSE}
ggplot(wine_copy, aes(x = color, y = pc1,group=color)) +geom_boxplot() + xlab("Color") + ylab("Principal Component 1 Score") + ggtitle("Principal Component 1 vs. Color")
```

We also checked if principal component 1 was good at distinguishing between wine qualities. Judging from the wide variation in principal component 1 scores for each level of quality, this does appear to be the case. 

*Figure 3*
```{r,echo=FALSE}
ggplot(wine_copy, aes(x = quality, y = pc1,group=color)) +geom_point() + xlab("Quality") + ylab("Principal Component 1 Score") + ggtitle("Principal Component 1 vs. Color")
```

To dive deeper into the meaning of the second principal component, we again looked at its correlations with other variables in the data set. We see that this principal component has strong positive correlations with: density, fixed.acidity, and residual.sugar. It also has strong negative correlations with alcohol. Yet again, having more domain knowledge about wine would be helpful here.

*Figure 4*
```{r,echo=FALSE}
wine_copy$pc2 <- wine_pca$x[,2]
cor_df_pc2 <- melt(as.data.frame(cor(wine_copy$pc2,wine_quant)))
colnames(cor_df_pc2) <- c("Variable","Correlation with PC2")
kable(cor_df_pc2)
```

Next, we examined whether principal component 2 was helpful to distinguish wine color as well as wine quality. It really wasn't too useful in labeling color or quality as shown in the plots below. There was a slight negative trend in Figure 6, but it wasn't very significant. We also tried principal components 3-5 (not shown in plots) and received similar results.

*Figure 5*
```{r,echo=FALSE}
ggplot(wine_copy, aes(x = color, y = pc2,group=color)) +geom_boxplot() + xlab("Color") + ylab("Principal Component 2 Score") + ggtitle("Principal Component 2 vs. Color")
```

*Figure 6*
```{r,echo=FALSE}
ggplot(wine_copy, aes(x = quality, y = pc2,group=color)) +geom_point() + xlab("Quality") + ylab("Principal Component 2 Score") + ggtitle("Principal Component 2 vs. Quality")
```

After thoroughly exploring PCA, we decided to move on to clustering. We used k-means clustering to create 2 clusters (hopefully one for red wines, and one for white). Our results were promising as Cluster 1 is mostly red wines, whereas Cluster 2 is mostly white wines. Even just making two clusters distinguishes between the two wine colors very well (98.58% of wines were grouped correctly).

*Figure 7*
```{r,echo=FALSE}
set.seed(1)
library(knitr)
wine_quant_scaled <- scale(wine_quant)
wine_clusters <- kmeans(wine_quant_scaled, centers=2, nstart=50)
kmeans_table <- table(wine_clusters$cluster,wine$color)
rownames(kmeans_table) <- c("Cluster 1","Cluster 2")
kable(kmeans_table)
```

While the 2 clusters separated out the two wine colors well, they don't seem to distinguish between wine quality because the median quality is essentially the same for both clusters. Even if we increase the number of clusters pretty dramatically up to 10, there still doesn't appear to be major quality differences between the box plots.

*Figure 2*
```{r,echo=FALSE}
wine$cluster = as.factor(wine_clusters$cluster)
ggplot(wine, aes(x = cluster, y = quality)) + geom_boxplot() + xlab("Cluster") + ylab("Wine Quality") + ggtitle("Wine Quality vs. Cluster Membership")
```

Overall, both PCA and k-means clustering could pretty easily distinguish red and white wines as shown in Figure 2 and Figure 7. Both methods weren't as good at determining the wine's quality. We think that k-means clustering is better primarily for ease of interpretation as it gives us one cluster label for each wine as opposed to multiple different principal components to try to then interpret their meaning.  

For our portfolio we selected five ETF's.

- ARKK (ARK Innovation) is a fund that invests in companies that are involved in the development of new technologies. (RISKY)
- QQQ (Invesco QQQ Trust) is a ETF with primarily US tech companies. (RISKY)
- IWM (iShares Russell 2000 ETF) is a ETF with primarily US small cap companies. (MODERATELY RISKY)
- SPY (SPDR S&P 500 ETF Trust) is a ETF with primarily US large cap companies. (MODERATELY RISKY)
- GLD (SPDR Gold Trust) is a ETF with primarily gold. (SAFE)

We tried to make three portfolios:

1. Higher weights in risky ETFs (0.4 ARKK, 0.2 QQQ, 0.15 IWM, 0.15 SPY, 0.1 GLD)
2. Equally weighted in all ETFs (0.2 ARKK, 0.2 QQQ, 0.2 IWM, 0.2 SPY, 0.2 GLD)
3. Lower weights in risky ETFs (0.1 ARKK, 0.2 QQQ, 0.2 IWM, 0.2 SPY, 0.3 GLD)


Theoretically portfolio (1) should have the highest VAR, followed by (2) and (3).
For our simulations we used 5000 iterations for each portfolio and daily data from 2017-2022.
We performed daily rebalancing in all simulations and have reported our VARs at the 95% confidence interval.

Results:
```{r, echo=FALSE}
set.seed(1)

# Get our Data --------------------------------------------------------#
mystocks <- c("ARKK", "QQQ", "IWM", "SPY", "GLD")

myprices <- getSymbols(mystocks, from = "2017-01-01", end = "2022-01-01")

for (ticker in mystocks) {
    expr <- paste0(ticker, "a = adjustOHLC(", ticker, ")")
    eval(parse(text = expr))
}

# Combine all the returns in a matrix
all_returns <- cbind(ClCl(ARKKa), ClCl(QQQa), ClCl(IWMa), ClCl(SPYa), ClCl(GLDa))
all_returns <- as.matrix(na.omit(all_returns))
# ---------------------------------------------------------------------#

# SIMULATION FUNCTION ------------------------------------------------#
# Make a function to simulate the returns with weight as input
simulate <- function(weights, n_days, initial_wealth, all_returns) {
    sim1 <- foreach(i = 1:5000, .combine = "rbind") %do% {
        total_wealth <- initial_wealth
        holdings <- weights * total_wealth
        wealthtracker <- rep(0, n_days)
        for (today in 1:n_days) {
            return_today <- resample(all_returns, 1, orig.ids = FALSE)
            holdings <- holdings + holdings * return_today
            total_wealth <- sum(holdings)
            wealthtracker[today] <- total_wealth
            # Rebalance the portfolio using weights
            holdings <- weights * total_wealth

        }
        wealthtracker
    }
    return(sim1)
}
# ---------------------------------------------------------------------#


# RUN SIMULATION -----------------------------------------------------#
n_days <- 20
initial_wealth <- 100000

# Simulation 1: Risky weighted more -----------------------#
weights <- c(0.4, 0.2, 0.15, 0.15, 0.1)
sim2 <- simulate(weights, n_days, initial_wealth, all_returns)
# plot normalized histogram
hist(sim2[, n_days] - initial_wealth, breaks = 30, main = "Risky Portfolio", xlab = "Profit / Loss", ylab = "Frequency") # nolint

# # 5% value at risk:
percentage_at_risk <- quantile(sim2[, n_days] - initial_wealth, prob = 0.05) / initial_wealth #nolint
value_at_risk <- quantile(sim2[, n_days] - initial_wealth, prob = 0.05)
print(paste("The value at risk for 4-weeks of the risky portfolio is ", round(value_at_risk, 2), "USD (", round(percentage_at_risk, 4) * 100, "%)")) #nolint
# Mean return:
mean_return <- mean(sim2[, n_days] - initial_wealth) / initial_wealth * 100
print(paste("The mean return for 4-weeks of the risky portfolio is ", round(mean_return, 4), "%")) #nolint


# Simulation 2: Equally weighted portfolio -----------------#
weights <- c(0.2, 0.2, 0.2, 0.2, 0.2)
sim1 <- simulate(weights, n_days, initial_wealth, all_returns)
hist(sim1[, n_days] - initial_wealth, breaks = 30, main = "Equally Weighted Portfolio", xlab = "Profit / Loss", ylab = "Frequency") # nolint

# # 5% value at risk:
percentage_at_risk <- quantile(sim1[, n_days] - initial_wealth, prob = 0.05) / initial_wealth #nolint
value_at_risk <- quantile(sim1[, n_days] - initial_wealth, prob = 0.05)
print(paste("The value at risk for 4-weeks of the equally weighted portfolio is ", round(value_at_risk, 2), "USD (", round(percentage_at_risk, 4) * 100, "%)")) #nolint
# Mean return:
mean_return <- mean(sim1[, n_days] - initial_wealth) / initial_wealth * 100
print(paste("The mean return for 4-weeks of the equally weighted portfolio is ", round(mean_return, 4), "%")) #nolint


# Simulation 3: Safe portfolio ----------------------------#
weights <- c(0.1, 0.15, 0.15, 0.2, 0.4)
sim3 <- simulate(weights, n_days, initial_wealth, all_returns)
# Plot histogram with title and axis labels
hist(sim3[, n_days] - initial_wealth, breaks = 30, main = "Safe Portfolio", xlab = "Profit / Loss", ylab = "Frequency") # nolint

# # 5% value at risk:
percentage_at_risk <- quantile(sim3[, n_days] - initial_wealth, prob = 0.05) / initial_wealth #nolint
value_at_risk <- quantile(sim3[, n_days] - initial_wealth, prob = 0.05)
print(paste("The value at risk for 4-weeks of the safe portfolio is ", round(value_at_risk, 2), "USD (", round(percentage_at_risk, 4) * 100, "%)")) #nolint
# Mean return:
mean_return <- mean(sim3[, n_days] - initial_wealth) / initial_wealth * 100
print(paste("The mean return for 4-weeks of the safe portfolio is ", round(mean_return, 4), "%")) #nolint
# ---------------------------------------------------------------------#
```

Comments:
As expected the Risky Portfolio has the highest value at risk 10.37%, followed by the Equally Weighted portfolio at 8.21%.<br>
Finally the Safe Portfolio has the lowest value at risk at 6.18%.<br>
We expected the portfolios with the highest VARs to have the highest returns as well (Risk-Return trade off).
This does hold true in our simulations with the 14-day mean returns being 1.6022%, 1.2601%, and 1.1514% for the Risky Portfolio, Equally Weighted Portfolio, and the Safe Portfolio respectively.<br>
<br>

# Market Segmentation

We started our analysis by looking at total counts of tweets per each tweet category. Among the top tweet categories, there appears to be a recurring theme regarding general health and wellness (health-nutrition, cooking, travel, personal fitness, and food). Given this result, we assumed that NutrientH20 is a "health and wellness" company. This directed our cluster analysis below to enhance NutrientH20's online advertising campaign strategy.

```{r,echo=FALSE}
#Read in data
social_mark <- read.csv("social_marketing.csv")
social_mark_melt <- melt(social_mark)
interest_totals <- social_mark_melt %>%
  group_by(variable) %>%
  summarize(total = sum(value))
ggplot(interest_totals, aes(x=reorder(variable,total),y=total)) + geom_bar(stat="identity",fill="cornflowerblue")+coord_flip()+xlab("Total Number of Tweets")+ylab("Tweet Category")+ggtitle("Total Number of Tweets per Tweet Category")
```

We decided to define market segments for this problem as clusters identified through the k-means clustering approach. We omitted the Twitter user's randomly generated ID when creating the clusters and instead only used the scores for each tweet interest area. We settled on creating 10 market segments (clusters) as 10 seemed to be a sweet spot between capturing legitimate differences between Twitter followers while also not overloading the company with too many market segments to try to understand.

```{r,include=FALSE}
#Set seed so we have reproducible results
set.seed(1)
#Drop the "X" column because we don't want to use the Twitter user ID in creating market segment clusters
social_mark_quant <- social_mark[,! names(social_mark) %in% "X"]
#Scale the other variables before doing k-means clustering
social_mark_quant_scaled <- scale(social_mark_quant)
#Run k-means, finding 10 clusters which we thought was a reasonably number of market segments to examine
social_mark_clusters <- kmeans(social_mark_quant_scaled, centers=10, nstart=50)
#Add each row's assigned cluster back to the original data frame
social_mark$cluster <- social_mark_clusters$cluster
#For each of the clusters, get the average score for each interest category
mkt_segments <- aggregate(social_mark[, 2:36], by=list(social_mark$cluster), mean)
mkt_segments <- melt(mkt_segments,id="Group.1")
mkt_segments <- mkt_segments %>% 
  group_by(Group.1) %>%
  arrange(desc(value)) %>% 
  slice(1:10)
```

```{r}
mkt_segments
```

While NutrientH20 might be interested in all 10 of the market segments that we've identified, they'll likely care the most about the segments that would be most receptive to their products. Thus, we found the 3 market segments with the highest average scores for the "health_nutrition" interest given that NutrientH20 seems to be a health-oriented company. The summaries of the three segments are below:

*Figure 1*
```{r,echo=FALSE}
#Extract only the scores for the health_nutrition interest
health_scores <- subset(mkt_segments, mkt_segments$variable=="health_nutrition")
#Get the top 3 average health_nutrition scores
top_health_scores <- subset(health_scores, value %in% tail(sort(value),3))
#Get the clusters corresponding to the top 3 average health_nutrition scores
top_health_clusters <- subset(mkt_segments, value %in% top_health_scores$value)
#Display all scores for these 3 clusters with the highest health_nutrition scores
top_health_clusters <- subset(mkt_segments, Group.1 %in%
top_health_clusters$Group.1)
colnames(top_health_clusters) = c("Market Segment","Interest Category","Average Score")
top_health_clusters <- as.data.frame(top_health_clusters)
segment1 = subset(top_health_clusters,top_health_clusters$`Market Segment`=="1")
segment3 = subset(top_health_clusters,top_health_clusters$`Market Segment`=="3")
segment4 = subset(top_health_clusters,top_health_clusters$`Market Segment`=="4")
kable(segment1)
```

*Figure 2*
```{r,echo=FALSE}
kable(segment3)
```

*Figure 3*
```{r,echo=FALSE}
kable(segment4)
```

From the three most promising market segments (highest scores for "health-nutrition"), the first one appears to be the most appealing to NutrientH20. Members of this segment have by far the highest average scores for the "health_nutrition" interest category, and also have high average scores for "fitness" which probably is something closely related to what NutrientH20 does as well. There are 768 Twitter users in the most promising market segment of segment 1, and then 475 and 49 in segments 3 and 4, respectively. Focusing in on these market segments will hopefully yield more future customers than trying to market to all Twitter followers.

*Figure 4*
```{r,echo=FALSE}
cluster_table <- as.data.frame(table(social_mark$cluster))
colnames(cluster_table) <- c("Cluster","Number of Twitter Users")
kable(cluster_table)
```


# The Reuters Corpus

**Figure out how to download this data**
- For now, just clone the Github and copy the folder over; I've added it to .gitignore so it won't be pushed to Github

```{r}

```

# Association Rule Mining

```{r,include=FALSE}
library(arules)
library(reshape2)
#Read in the groceries.txt file. Find max number of objects
#in a basket so that R doesn't automatically cap the number
#of columns we can have
no_col <- max(count.fields("groceries.txt", sep = ","))
groceries <- read.table("groceries.txt",sep=",",fill=TRUE,col.names=c(1:no_col))
no_col <- max(count.fields("data/groceries.txt", sep = ","))
groceries <- read.table("data/groceries.txt",sep=",",fill=TRUE,col.names=c(1:no_col))
#Add in a column that indicates which customer corresponds to 
#the basket (row number)
groceries$customer = as.factor(1:nrow(groceries))
#Get data in long format
groceries <- melt(groceries,id.vars = 'customer') 
groceries <- as.data.frame(groceries)
#Drop all values that are blank
groceries <- subset(groceries, groceries$value != "")
#Group the grocery products by the customer who bought them
groceries <- split(x=groceries$value, f=groceries$customer)
#Make sure each customer is only associated with unique
#products in their basket
groceries <- lapply(groceries, unique)
#Change our data into a "transactions" type variable so that we can supply it to the apriori() function
groceries_trans <- as(groceries,"transactions")
#Run the apriori function with values for support, confidence, and maxlen that we played around with until we got a set of rules that we thought were reasonably useful
groceries_rules <- apriori(groceries_trans, 
	parameter=list(support=.005, confidence=.4, maxlen=4))
```

We decided to make a set of association rules for the groceries data that had a support of at least 0.005, a confidence of at least 0.4, and a maximum length of 4. We felt as though rules conforming to these thresholds were reasonably common enough (in 0.5% of all baskets) to not just be flukes and had high enough conditional probabilities to potentially be actionable. We also capped the maximum length to be 4 to avoid any long chains of items in our association rules that would lack interpretability.

Once these association rules were developed, we wanted to visualize the top 10 rules in terms of lift. What's immediately apparent in the plot below is there are three right hand side items: tropical fruit, root vegetables, and yogurt. Among these three items, root vegetables have 6/10 of the top 10 rules. This isn't all that helpful as these 6 rules could help us sell more root vegetables, but it also could be the case that lots of people buy root vegetables in general which is why it appears in so many of these top lift rules. Additionally, the maximum lift was only about 4.08 which isn't very large especially compared to our playlist analysis in class.

*Figure 1*
```{r,echo=FALSE}
library(arulesViz)
interesting_rules <- head(sort(groceries_rules, by="lift"), 10)
plot(interesting_rules, method="grouped")
```

To check the overall popularity of the three right hand side items above, we created the plot below of the top 10 items across all of our association rules. Whole milk (80%) and other vegetables (45%) are the most common items in our association rules as shown by the bars. The red line shows the overall frequency of the items across all shopping baskets. The three right hand side items from above (tropical fruit, root vegetables, and yogurt) all appeared slightly more frequently in our association rules than they did in overall market basket transactions. This plot also reveals that outside of the top 10 lift association rules, whole milk dominates the association rules as it appears in more than 80% of them.

*Figure 2* - **Relative Frequency of Top 10 Items in Association Rules**

```{r,echo=FALSE}
itemFrequencyPlot(items(groceries_rules),population=groceries_trans,topN=10,popCol="red",xlab="Item",ylab="Relative Frequency")
```

```{r,include=FALSE}
whole_milk_rules <- apriori(groceries_trans, 
	parameter=list(support=.005, confidence=.4, maxlen=4),appearance = list(default="lhs", rhs="whole milk"))
length(whole_milk_rules)/length(groceries_rules)
```

After some further analysis, we found that whole milk appears on the right in about $\approx$ 63% of these association rules. This is likely because of milk's relatively high prevalence in all market baskets. Furthermore, more accurately predicting who will buy milk isn't all that useful given that milk is a low cost staple item unlikely to greatly enhance a store's revenue with higher sales.

The last step of our analysis was to import this data into Gephi to try and visualize any patterns between items that we missed via the plots above.

```{r,include=FALSE}
#Export to a graphml file so that we can visualize this data in Gephi
library(igraph)
groceries_graph = associations2igraph(subset(groceries_rules, lift>1), associationsAsNodes = FALSE)
igraph::write_graph(groceries_graph, file='groceries.graphml', format = "graphml")
```

In the figure below, we created a network plot that visualizes the relationships between the various items in our association rules with items with a higher degree (i.e. more connections) in larger font. We also used the modularity feature to group the items into two groups, although the groups themselves aren't that interesting. It seems as though the red group is items that are associated with milk, and the green group is items that are associated with other vegetables. We saw in Figure 2 that whole milk and other vegetables appeared the most frequently in our association rules and unsurprisingly, we find them at the center of this network plot in the largest font.

*Figure 3* - **Network Plot of Association Rules Colored by Modularity**

![alt](Assoc Rules 1.png)

We then decided to look more in depth at yogurt because it was one of the top right hand side items in Figure 1. We felt like the connections in Figure 4 below for yogurt made sense which validated our network plot. Yogurt appears in association rules alongside other dairy products such as: whipped/sour cream, cream cheese, and curds. This seems logical since these items are likely placed in the same area so shoppers are tempted to buy them together in greater frequencies. Furthermore, yogurt also appeared in association rules with whole milk and produce because these were very commonly bought items overall.

*Figure 4* - **Network Plot Connections of Yogurt**

![alt](Assoc Rules 2.png)
