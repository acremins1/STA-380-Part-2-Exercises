---
title: 'STA 380 Part 2: Exercises'
author: "Aidan Cremins, Peyton Lewis, Joe Morris, Amrit Sandhu"
date: '2022-07-29'
output: rmarkdown::github_document
---

```{r}
library(dplyr)
library(ggplot2)
library(forcats)
```


# Probability Practice

### Part a.

P(Y) = 0.65
P(N) = 0.35
P(RC) = 0.3
P(TC) = 0.7 (P(RC)-1)
P(Y|RC) = 0.5
P(N|RC) = 0.5

We're looking for P(Y|TC) so we can use the rule of total probability:

P(Y) = P(Y, TC) + P(Y, RC) = P(TC) * P(Y|TC) + P(RC) * P(Y|RC)

We know all of these inputs to the equation except for P(Y|TC), so we want to solve for that unknown.

0.65 = 0.7 * P(Y|TC) + 0.3 * 0.5

From the above equation, we find that P(Y|TC) $\approx$ 0.714286. This means that truthful clickers answer yes to the question about 71.43% of the time.

$\frac{P(Y)*P(TC|Y)}{P(Yes)*P(TC|Y)+P(No)*P(RC|Y)}$

$\frac{0.65*0.5}{.7*0.5+0.3*0.5}$

### Part b.

P(Positive|Disease) = .993
P(Negative|No Disease) = 0.9999
P(Disease) = 0.000025
P(No Disease) = 0.999975 (1-0.000025)

We're looking for P(Disease|Positive) so we can use Baye's Law:

$\frac{P(Disease)*P(Positive|Disease)}{P(Disease)*P(Positive|Disease)+P(No Disease)*P(Positive|No Disease)}$

We have almost all of the inputs that we need, however, we're missing P(Positive|No Disease). These are false positives. We can find the missing probability by taking 1 - true negatives, or 1 - 0.9999 to get P(Positive|No Disease) as 0.0001. Now we can solve for P(Disease|Positive).

$\frac{0.000025*0.993}{0.000025*0.993+0.999975*0.0001}$ $\approx$ .198882. Thus, if someone tests positive, they have about a 19.89% chance of actually having the disease.

# Wrangling the Billboard Top 100

```{r}
billboard = read.csv("data/billboard.csv")
```

#Need a caption - probably something about how most are recent songs

### Part a.

```{r}
billboard %>%
  group_by(performer, song) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  head(10)
```

### Part b.

```{r}
musical_diversity = billboard %>%
  filter(year != 1958 & year != 2021) %>%
  group_by(year) %>%
  summarize(unique_songs_per_year = length(unique(c(performer,song))))

ggplot(musical_diversity) + geom_line(aes(x = year, y = unique_songs_per_year))
```

### Part c.

```{r}
ten_week_hit_songs <- billboard %>%
  group_by(performer,song) %>%
  summarize(ten_week_hit = ifelse(n()>=10,"Yes","No")) %>%
  filter(ten_week_hit == "Yes")

top_artists <- ten_week_hit_songs %>%
  group_by(performer) %>%
  summarize(num_ten_week_hit = n()) %>%
  filter(num_ten_week_hit>=30)

ggplot(top_artists) + geom_bar(aes(x = fct_reorder(performer,num_ten_week_hit), y = num_ten_week_hit),stat = "identity") + coord_flip()
```

#Visual story telling part 1: green buildings

```{r}
green_buildings = read.csv("data/greenbuildings.csv")
```

```{r}
ggplot(green_buildings, aes(x = Rent)) + geom_histogram() + facet_grid(.~green_rating)
```

Green buildings tend to be newer, "newness" could justify higher rents
```{r}
ggplot(green_buildings, aes(x = green_rating, y = age ,group = green_rating)) + geom_boxplot()
```

```{r}
ggplot(green_buildings, aes(x = age, y = Rent)) + geom_line() + facet_grid(. ~ green_rating)
```

# Visual story telling part 2: Cap Metro data

```{r}
cap_metro <- read.csv("data/capmetro_UT.csv")
```
```{r}
cap_metro$time_of_day = ifelse(cap_metro$hour_of_day %in% c(6,7,8,9,10,11),"Morning",ifelse(cap_metro$hour_of_day %in% c(12,13,14,15,16),"Afternoon","Evening"))
time_of_day_order <- c("Morning","Afternoon","Evening")
cap_metro$activity = cap_metro$boarding + cap_metro$alighting
ggplot(cap_metro, aes(x = factor(time_of_day,levels=time_of_day_order), y = activity)) + geom_bar(stat="identity") + facet_grid(. ~ weekend)
```

Activity seems to slightly increase as temperature increases, adjusted for the difference in ridership between weekdays and weekends
```{r}
riders_temp = cap_metro %>%
  group_by(timestamp) %>%
  summarize(total_riders = sum(activity), mean_temp = mean(temperature), weekend = weekend)
ggplot(riders_temp, aes(x = mean_temp, y = total_riders)) + geom_line()+facet_grid(.~weekend)
```

# Clustering and PCA

```{r}
wine <- read.csv("data/wine.csv")
```

```{r}
set.seed(1)
wine_quant <- wine[,! names(wine) %in% c("color","quality")]
wine_pca = prcomp(wine_quant, rank=10, scale=TRUE)
boxplot(wine_pca$x[,1],as.factor(wine$color))
```

Cluster 1 is mostly red wines, whereas Cluster 2 is mostly white wines. Even just making two clusters distinguishes between the two wine colors very well. 
```{r}
set.seed(1)
library(knitr)
wine_quant_scaled <- scale(wine_quant)
wine_clusters <- kmeans(wine_quant_scaled, centers=2, nstart=50)
table(wine_clusters$cluster,wine$color)
```

While the 2 clusters separated out the two wine colors well, they don't seem to distinguish between wine quality because the median quality is essentially the same for both clusters. Even if we increase the number of clusters pretty dramatically up to 10, there still doesn't appear to be major quality differences between the boxplots.
```{r}
wine$cluster = as.factor(wine_clusters$cluster)
ggplot(wine, aes(x = cluster, y = quality)) + geom_boxplot()
```

# Market Segmentation

We decided to define market segments for this problem as clusters identified through the k-means clustering approach. We omitted the Twitter user's randomly generated ID when creating the clusters and instead only used the scores for each Tweet interest. We settled on creating 10 market segments (clusters) as 10 seemed to be a sweet spot between capturing legitimate differences between Twitter followers while also not overloading the company with too many market segments to try to understand.

```{r,include=FALSE}
library(reshape2)
#Set seed so we have reproducible results
set.seed(1)
#Read in data
social_mark <- read.csv("data/social_marketing.csv")
#Drop the "X" column because we don't want to use the Twitter user ID in creating market segment clusters
social_mark_quant <- social_mark[,! names(social_mark) %in% "X"]
#Scale the other variables before doing kmeans clustering
social_mark_quant_scaled <- scale(social_mark_quant)
#Run kmeans, finding 10 clusters which we thought was a reasonably number of market segments to examine
social_mark_clusters <- kmeans(social_mark_quant_scaled, centers=10, nstart=50)
#Add each row's assigned cluster back to the original data frame
social_mark$cluster <- social_mark_clusters$cluster
#For each of the clusters, get the average score for each interest category
mkt_segments <- aggregate(social_mark[, 2:36], by=list(social_mark$cluster), mean)
mkt_segments <- melt(mkt_segments,id="Group.1")
mkt_segments <- mkt_segments %>% 
  group_by(Group.1) %>%
  arrange(desc(value)) %>% 
  slice(1:10)
```

While NutrientH20 might be interested in all 10 of the market segments that we've identified, they'll likely care the most about the segments that would be most receptive to their products. Thus, we found the 3 market segments with the highest average scores for the "health_nutrition" interest given that NutrientH20 seems to be a health-oriented company. The summaries of the three segments are below:

```{r,echo=FALSE}
#Extract only the scores for the health_nutrition interest
health_scores <- subset(mkt_segments, mkt_segments$variable=="health_nutrition")
#Get the top 3 average health_nutrition scores
top_health_scores <- subset(health_scores, value %in% tail(sort(value),3))
#Get the clusters corresponding to the top 3 average health_nutrition scores
top_health_clusters <- subset(mkt_segments, value %in% top_health_scores$value)
#Display all scores for these 3 clusters with the highest health_nutrition scores
top_health_clusters <- subset(mkt_segments, Group.1 %in%
top_health_clusters$Group.1)
colnames(top_health_clusters) = c("Market Segment","Interest Category","Average Score")
top_health_clusters <- as.data.frame(top_health_clusters)
top_health_clusters
```

From the three most promising market segments, the first one appears to be the most appealing to NutrientH20. Members of this segment have by far the highest average scores for the "health_nutrition" interst category, and also have high average scores for "fitness" which probably is something closely related to what NutrientH20 does as well. There are 768 Twitter users in the most promising market segment of segment 1, and then 475 and 49 in segments 3 and 4, respectively. Focusing in on these market segment will hopefully yield more future customers than trying to market to all Twitter followers.

# The Reuters Corpus

**Figure out how to download this data**
- For now, just clone the Github and copy the folder over; I've added it to .gitignore so it won't be pushed to Github

```{r}

```

# Association Rule Mining

```{r}
library(arules)
library(reshape2)
#Read in the groceries.txt file. Find max number of objects
#in a basket so that R doesn't automatically cap the number
#of columns we can have
no_col <- max(count.fields("data/groceries.txt", sep = ","))
groceries <- read.table("data/groceries.txt",sep=",",fill=TRUE,col.names=c(1:no_col))
#Add in a column that indicates which customer corresponds to 
#the basket (row number)
groceries$customer = as.factor(1:nrow(groceries))
#Get data in long format
groceries <- melt(groceries,id.vars = 'customer') 
groceries <- as.data.frame(groceries)
#Drop all values that are blank
groceries <- subset(groceries, groceries$value != "")
#Group the grocery products by the customer who bought them
groceries <- split(x=groceries$value, f=groceries$customer)
#Make sure each customer is only associated with unique
#products in their basket
groceries <- lapply(groceries, unique)
```

```{r,include=FALSE}
#Change our data into a "transactions" type variable so that we can supply it to the apriori() function
groceries_trans <- as(groceries,"transactions")
#Run the apriori function with values for support, confidence, and maxlen that we played around with until we got a set of rules that we thought were reasonably useful
groceries_rules <- apriori(groceries_trans, 
	parameter=list(support=.01, confidence=.4, maxlen=4))
```

```{r}
#Export to a graphml file so that we can visualize this data in Gephi
library(igraph)
library(arulesViz)
groceries_graph = associations2igraph(subset(groceries_rules, lift>1), associationsAsNodes = FALSE)
igraph::write_graph(groceries_graph, file='groceries.graphml', format = "graphml")
```